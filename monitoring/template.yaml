apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: prometheus
  annotations:
    "openshift.io/display-name": Prometheus
    description: |
      A monitoring solution for an OpenShift cluster - collect and gather metrics and alerts from nodes, services, and the infrastructure.
    iconClass:  icon-codeigniter
    tags: "monitoring,prometheus, alertmanager,time-series"
parameters:
- description: The namespace to instantiate prometheus under. Defaults to 'kube-system'.
  name: NAMESPACE
  required: true
#  value: kube-system
- description: The location of the proxy image
  name: IMAGE_PROXY
  value: registry.redhat.io/openshift3/oauth-proxy:v3.11
- description: The location of the prometheus image
  name: IMAGE_PROMETHEUS
  value: quay.io/prometheus/prometheus:v2.9.2
- description: The location of the alertmanager image
  name: IMAGE_ALERTMANAGER
  value: quay.io/prometheus/alertmanager:v0.16.2
- description: The location of the blackbox exporter image
  name: IMAGE_BLACKBOX_EXPORTER
  value: quay.io/prometheus/blackbox-exporter:v0.14.0
- description: The location of the kube state metrics  image
  name: KUBE_STATE_METRICS
  value: quay.io/coreos/kube-state-metrics:v1.6.0
- description: The session secret for the alert manager proxy
  name: ALERTMANAGER_SESSION_SECRET
  generate: expression
  from: "[a-zA-Z0-9]{43}"
- description: The session secret for the prometheus proxy
  name: PROMETHEUS_SESSION_SECRET
  generate: expression
  from: "[a-zA-Z0-9]{43}"
- description: Wildcard domain (e.g. apps.example.com)
  name: WILDCARD_DOMAIN
  required: true
  value: apps.example.com
objects:
# Authorize the prometheus service account to read data about the cluster
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
    annotations:
      serviceaccounts.openshift.io/oauth-redirectreference.prom: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"prometheus"}}'
      serviceaccounts.openshift.io/oauth-redirectreference.alerts: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"alertmanager"}}'
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: kube-state-metrics
    namespace: "${NAMESPACE}"

- apiVersion: authorization.openshift.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: prometheus-cluster-reader
  roleRef:
    name: system:auth-delegator
  subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: "${NAMESPACE}"
- apiVersion: authorization.openshift.io/v1
  groupNames: null
  kind: ClusterRoleBinding
  metadata:
    name: cluster-reader-prometheus
  roleRef:
    name: cluster-reader
  subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: "${NAMESPACE}"
  userNames:
  - "system:serviceaccount:${NAMESPACE}:prometheus"

- apiVersion: authorization.openshift.io/v1
  groupNames: null
  kind: ClusterRoleBinding
  metadata:
    name: kube-state-metrics-monitoring
  roleRef:
    name: kube-state-metrics
  subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: "${NAMESPACE}"
  userNames:
  - "system:serviceaccount:${NAMESPACE}:kube-state-metrics"

- apiVersion: authorization.openshift.io/v1
  kind: RoleBinding
  metadata:
    name: prometheus-viewer
  roleRef:
    name: view
  subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: "${NAMESPACE}"
# Create a fully end-to-end TLS connection to the prometheus proxy
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    host: "prometheus-${NAMESPACE}.${WILDCARD_DOMAIN}"
    to:
      name: prometheus
    port:
      targetPort: prometheus-https
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/scheme: https
      service.alpha.openshift.io/serving-cert-secret-name: prometheus-tls
    labels:
      name: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: prometheus-https
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: prometheus
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/scheme: http
    labels:
      name: prometheus
    name: prometheus-direct
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: prometheus-direct
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app: prometheus
- apiVersion: v1
  kind: Secret
  metadata:
    name: prometheus-proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${PROMETHEUS_SESSION_SECRET}="


- apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: kube-state-metrics
    name: kube-state-metrics
    namespace: "${NAMESPACE}"
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: '8080'
          prometheus.io/scrape: 'true'
        labels:
          app: kube-state-metrics
      spec:
        containers:
          - image: "${KUBE_STATE_METRICS}"
            imagePullPolicy: IfNotPresent
            name: kube-rbac-proxy-main
            ports:
              - containerPort: 8443
                name: https-main
                protocol: TCP
            resources:
              limits:
                cpu: 200m
                memory: 400Mi
              requests:
                cpu: 10m
                memory: 20Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-state-metrics
        serviceAccountName: kube-state-metrics
        terminationGracePeriodSeconds: 30

- apiVersion: apps.openshift.io/v1
  kind: DeploymentConfig
  metadata:
    creationTimestamp: null
    generation: 1
    labels:
      app: blackbox-exporter
    name: blackbox-exporter
  spec:
    replicas: 1
    revisionHistoryLimit: 5
    selector:
      app: blackbox-exporter
      deploymentconfig: blackbox-exporter
    strategy:
      activeDeadlineSeconds: 21600
      resources: {}
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: blackbox-exporter
          deploymentconfig: blackbox-exporter
      spec:
        containers:
        - image: "${IMAGE_BLACKBOX_EXPORTER}"
          imagePullPolicy: IfNotPresent
          livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 9115
            scheme: HTTP
          initialDelaySeconds: 2
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
          name: blackbox-exporter
          ports:
          - containerPort: 9115
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9115
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
    test: false
    triggers:
    - type: ConfigChange

- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: blackbox-exporter
    name: blackbox-exporter
  spec:
    ports:
    - name: 9115-tcp
      port: 9115
      protocol: TCP
      targetPort: 9115
    selector:
      deploymentconfig: blackbox-exporter
    sessionAffinity: None
    type: ClusterIP

- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      service.alpha.openshift.io/serving-cert-secret-name: alertmanager-tls
    labels:
      name: alertmanager
    name: alertmanager-tls
    namespace: ${NAMESPACE}
  spec:
    ports:
      - name: web
        port: 443
        protocol: TCP
        targetPort: 8443
    selector:
      app: alertmanager
    sessionAffinity: None
    type: ClusterIP


- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: alertmanager
    name: alertmanager-operated
    namespace: ${NAMESPACE}
  spec:
    clusterIP: None
    ports:
      - name: web
        port: 9093
        protocol: TCP
        targetPort: 9093
      - name: mesh
        port: 6783
        protocol: TCP
        targetPort: 6783
    selector:
      app: alertmanager
    sessionAffinity: None
    type: ClusterIP

- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    generation: 3
    labels:
      app: alertmanager
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    podManagementPolicy: OrderedReady
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: alertmanager
      spec:
        containers:
          - args:
              - '--config.file=/etc/alertmanager/config.yml'
              - '--cluster.listen-address=$(POD_IP):6783'
              - >-
                --cluster.peer=alertmanager-0.alertmanager-operated.${NAMESPACE}.svc.cluster.local:6783
              - >-
                --cluster.peer=alertmanager-1.alertmanager-operated.${NAMESPACE}.svc.cluster.local:6783
              - '--storage.path=/alertmanager'
              - '--data.retention=120h'
              - >-
                --web.external-url=https://alertmanager-${NAMESPACE}.${WILDCARD_DOMAIN}
            env:
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    apiVersion: v1
                    fieldPath: status.podIP
            image: ${IMAGE_ALERTMANAGER}
            imagePullPolicy: IfNotPresent
            name: alertmanager
            ports:
              - containerPort: 6783
                name: mesh
                protocol: TCP
            resources:
              requests:
                memory: 200Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /etc/alertmanager
                name: config-volume
              - mountPath: /alertmanager
                name: data
          - args:
              - '-provider=openshift'
              - '-https-address=:8443'
              - '-http-address='
              - '-email-domain=*'
              - '-upstream=http://localhost:9093'
              - '-client-id=system:serviceaccount:${NAMESPACE}:prometheus'
              - '-openshift-ca=/etc/pki/tls/cert.pem'
              - '-openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
              - >-
                -openshift-sar={"resource": "namespaces", "verb": "get",
                "resourceName": "pods", "namespace": "${NAMESPACE}"}
              - '-tls-cert=/etc/tls/private/tls.crt'
              - '-tls-key=/etc/tls/private/tls.key'
              - >-
                -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
              - '-cookie-secret-file=/etc/proxy/secrets/session_secret'
              - '-skip-auth-regex=^/metrics'
            image: ${IMAGE_PROXY}
            imagePullPolicy: IfNotPresent
            name: alertmanager-proxy
            ports:
              - containerPort: 8443
                name: web
                protocol: TCP
            resources:
              limits:
                cpu: 100m
                memory: 200Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /etc/tls/private
                name: alertmanager-tls
              - mountPath: /etc/proxy/secrets
                name: alertmanager-secrets
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/compute: 'true'
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus
        serviceAccountName: prometheus
        terminationGracePeriodSeconds: 0
        volumes:
          - configMap:
              defaultMode: 420
              name: alertmanager
            name: config-volume
          - name: alertmanager-tls
            secret:
              defaultMode: 420
              secretName: alertmanager-tls
          - name: alertmanager-secrets
            secret:
              defaultMode: 420
              secretName: alertmanager-proxy
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
      - metadata:
          creationTimestamp: null
          labels:
            app: alertmanager
          name: data
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi
        status:
          phase: Pending

- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    labels:
      app: prometheus
    name: prometheus
    namespace: ${NAMESPACE}
  spec:
    podManagementPolicy: OrderedReady
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: prometheus
    serviceName: prometheus
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
      spec:
        containers:
          - args:
              - '--storage.tsdb.retention=6h'
              - '--storage.tsdb.min-block-duration=2m'
              - '--config.file=/etc/prometheus/prometheus.yml'
              - '--web.enable-lifecycle'
              - >-
                --web.external-url=https://prometheus-${NAMESPACE}.${WILDCARD_DOMAIN}
              - '--storage.tsdb.no-lockfile'
              - '--web.route-prefix=/'
            image: '${IMAGE_PROMETHEUS}'
            imagePullPolicy: IfNotPresent
            name: prometheus
            ports:
              - containerPort: 9090
                name: prometheus
                protocol: TCP
            resources:
              limits:
                cpu: 400m
                memory: 1Gi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /etc/prometheus
                name: prometheus-config
              - mountPath: /prometheus
                name: data
          - args:
              - '-provider=openshift'
              - '-https-address=:8443'
              - '-http-address='
              - '-email-domain=*'
              - '-upstream=http://localhost:9090'
              - '-client-id=system:serviceaccount:${NAMESPACE}:prometheus'
              - '-openshift-ca=/etc/pki/tls/cert.pem'
              - '-openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
              - >-
                -openshift-sar={"resource": "namespaces", "verb": "get",
                "resourceName": "pods", "namespace": "${NAMESPACE}"}
              - '-tls-cert=/etc/tls/private/tls.crt'
              - '-tls-key=/etc/tls/private/tls.key'
              - >-
                -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
              - '-cookie-secret-file=/etc/proxy/secrets/session_secret'
              - '-skip-auth-regex=^/metrics'
            image: '${IMAGE_PROXY}'
            imagePullPolicy: IfNotPresent
            name: prometheus-proxy
            ports:
              - containerPort: 8443
                name: web
                protocol: TCP
            resources:
              limits:
                cpu: 100m
                memory: 200Mi
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /etc/tls/private
                name: prometheus-tls
              - mountPath: /etc/proxy/secrets
                name: prometheus-secrets
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/compute: 'true'
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus
        serviceAccountName: prometheus
        terminationGracePeriodSeconds: 600
        volumes:
          - name: prometheus-secrets
            secret:
              defaultMode: 420
              secretName: prometheus-proxy
          - name: prometheus-tls
            secret:
              defaultMode: 420
              secretName: prometheus-tls
          - configMap:
              defaultMode: 420
              name: prometheus
            name: prometheus-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
      - metadata:
          creationTimestamp: null
          labels:
            app: prometheus
          name: data
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 50Gi
        status:
          phase: Pending

- apiVersion: v1
  data:
    config.yml: |-
      global:

      # The root route on which each incoming alert enters.
      route:
        # The root route must not have any matchers as it is the entry point for
        # all alerts. It needs to have a receiver configured so alerts that do not
        # match any of the sub-routes are sent to someone.
        receiver: 'webhook'

        # The labels by which incoming alerts are grouped together. For example,
        # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
        # be batched into a single group.
        group_by: ['alertname', 'cluster']

        # When a new group of alerts is created by an incoming alert, wait at
        # least 'group_wait' to send the initial notification.
        # This way ensures that you get multiple alerts for the same group that start
        # firing shortly after another are batched together on the first
        # notification.
        group_wait: 30s

        # When the first notification was sent, wait 'group_interval' to send a batch
        # of new alerts that started firing for that group.
        group_interval: 5m

        # If an alert has successfully been sent, wait 'repeat_interval' to
        # resend them.
        repeat_interval: 3h

      receivers:
      - name: 'webhook'
        webhook_configs:
        - url: https://webhook.site/5b475278-bd51-4c3d-b1d9-33b2914e3a5c
  kind: ConfigMap
  metadata:
    name: alertmanager
    namespace: ${NAMESPACE}
- apiVersion: v1
  data:
    prometheus.rules: |+
      groups:
      - name: example-rules
        interval: 30s # defaults to global interval
        rules:
        - alert: Node Down
          expr: up{job="kubernetes-nodes"} == 0
          annotations:
            miqTarget: "ContainerNode"
            severity: "HIGH"
            message: "{{$labels.instance}} is down"
        - alert: ReplicaCount
          expr: count(up{app="java-app"}) < 3
          annotations:
            miqTarget: "ReplicaCount"
            severity: "CRITICAL"
            message: "{{$labels.instance}} has less than 3 replicas"

    prometheus.yml: "rule_files:\n  - 'prometheus.rules'\n\n# A scrape configuration
      for running Prometheus on a Kubernetes cluster.\n# This uses separate scrape
      configs for cluster components (i.e. API server, node)\n# and services to allow
      each to use different authentication configs.\n#\n# Kubernetes labels will be
      added as Prometheus labels on metrics via the\n# `labelmap` relabeling action.\n\n#
      Scrape config for API servers.\n#\n# Kubernetes exposes API servers as endpoints
      to the default/kubernetes\n# service so this uses `endpoints` role and uses
      relabelling to only keep\n# the endpoints associated with the default/kubernetes
      service using the\n# default named port `https`. This works for single API server
      deployments as\n# well as HA API server deployments.\nscrape_configs:\n- job_name:
      'kubernetes-pods'\n\n  kubernetes_sd_configs:\n  - role: pod\n    namespaces:\n
      \     names:\n      - ${NAMESPACE}\n      - java-test\n\n  relabel_configs:\n
      \ - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n
      \   action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n
      \   action: replace\n    target_label: __metrics_path__\n    regex: (.+)\n  -
      source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n
      \   action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n
      \   target_label: __address__\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n
      \ - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label:
      kubernetes_namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action:
      replace\n    target_label: kubernetes_pod_name\n\n# Scrape config for service
      endpoints.\n#\n# The relabeling allows the actual service scrape endpoint to
      be configured\n# via the following annotations:\n#\n# * `prometheus.io/scrape`:
      Only scrape services that have a value of `true`\n# * `prometheus.io/scheme`:
      If the metrics endpoint is secured then you will need\n# to set this to `https`
      & most likely set the `tls_config` of the scrape config.\n# * `prometheus.io/path`:
      If the metrics path is not `/metrics` override this.\n# * `prometheus.io/port`:
      If the metrics are exposed on a different port to the\n# service then set this
      appropriately.\n- job_name: 'kubernetes-service-endpoints'\n\n  tls_config:\n
      \   ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    # TODO:
      this should be per target\n    insecure_skip_verify: true\n\n  kubernetes_sd_configs:\n
      \ - role: endpoints\n    namespaces:\n      names:\n      - ${NAMESPACE}\n\n  relabel_configs:\n
      \ - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n
      \   action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n
      \   action: replace\n    target_label: __scheme__\n    regex: (https?)\n  -
      source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n    action:
      replace\n    target_label: __metrics_path__\n    regex: (.+)\n  - source_labels:
      [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n    action:
      replace\n    target_label: __address__\n    regex: (.+)(?::\\d+);(\\d+)\n    replacement:
      $1:$2\n  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_username]\n
      \   action: replace\n    target_label: __basic_auth_username__\n    regex: (.+)\n
      \ - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_password]\n
      \   action: replace\n    target_label: __basic_auth_password__\n    regex: (.+)\n
      \ - action: labelmap\n    regex: __meta_kubernetes_service_label_(.+)\n  - source_labels:
      [__meta_kubernetes_namespace]\n    action: replace\n    target_label: kubernetes_namespace\n
      \ - source_labels: [__meta_kubernetes_service_name]\n    action: replace\n    target_label:
      kubernetes_name\n\n# Example scrape config for probing services via the Blackbox
      Exporter.\n#\n# The relabeling allows the actual service scrape endpoint to
      be configured\n# via the following annotations:\n#\n# * `prometheus.io/probe`:
      Only probe services that have a value of `true`\n- job_name: 'kubernetes-services'\n\n
      \ tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n
      \   # TODO: this should be per target\n    insecure_skip_verify: true\n\n  metrics_path:
      /metrics\n  params:\n    module: [http_2xx]\n\n  kubernetes_sd_configs:\n  -
      role: service\n    namespaces:\n      names:\n      - ${NAMESPACE}\n\n  relabel_configs:\n
      \ - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n
      \   action: keep\n    regex: true\n  - source_labels: [__address__]\n    target_label:
      __param_target\n  - target_label: __address__\n    replacement: blackbox-exporter.${NAMESPACE}.svc:9115\n
      \ - source_labels: [__param_target]\n    target_label: instance\n  - action:
      labelmap\n    regex: __meta_kubernetes_service_label_(.+)\n  - source_labels:
      [__meta_kubernetes_namespace]\n    target_label: kubernetes_namespace\n  - source_labels:
      [__meta_kubernetes_service_name]\n    target_label: kubernetes_name\n    \nalerting:\n
      \ alertmanagers:\n  - scheme: http\n    static_configs:\n    - targets:\n      -
      \"alertmanager.${NAMESPACE}.svc:9093\"\n"
  kind: ConfigMap
  metadata:
    name: prometheus
    namespace: ${NAMESPACE}

# Create a fully end-to-end TLS connection to the alertmanager proxy
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    host: "alertmanager-${NAMESPACE}.${WILDCARD_DOMAIN}"
    to:
      name: alertmanager-tls
    port:
      targetPort: web
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect

- apiVersion: v1
  kind: Service
  metadata:
    labels:
      name: alertmanager
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: alertmanager
      port: 9093
      protocol: TCP
      targetPort: 9093
    selector:
      app: alertmanager
- apiVersion: v1
  kind: Secret
  metadata:
    name: alertmanager-proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${ALERTMANAGER_SESSION_SECRET}="
